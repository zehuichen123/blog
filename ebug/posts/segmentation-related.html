
<!doctype>
<html lang="en">
  <head>
    <meta content='Segmentation Related Models and Loss - Zehui Chen' name='title' />
    <meta content='Segmentation Related Models and Loss - Zehui Chen' name='og:title' />
    <title>Segmentation Related Models and Loss - Zehui Chen</title>
    <link href='http://localhost:4000/images/fav.png' rel='shortcut icon'>
<link href='http://localhost:4000/stylesheets/style.css' rel='stylesheet' type='text/css' />
<link href='http://localhost:4000/stylesheets/syntax.css' rel='stylesheet' type='text/css' />

<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type' />

  <meta content='http://localhost:4000/posts/segmentation-related' property='og:url' />
  <meta content="In this post, we will cover about some techniques about image segmentation, from FCN to DeepLab, from cross entropy l..." property='og:description' />
  <meta content="article" property="og:type" />

<!-- - -->





    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$','$']]
        }
      });
    </script> <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </head>
  <body class="lh-copy dark-gray pa0 f6 sans-serif bg-super-white">
    <header class="tc mt4">
      <a href="http://localhost:4000">
        <img src="http://localhost:4000/images/new.jpg" alt="Home" width="70" height="70" style="border-radius:50%;">
      </a>
      <p>Zehui Chen</p>
    </header>
    <div class="mw8 bg-white mt4 mb3 center br2-ns bt bb ba-ns b--light-gray" style="max-width: 59rem">
      <nav class="bb b--light-gray pv4 tc" aria-label="Main">
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/about">
             About
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/">
             Blog
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/friends">
             Friends
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="https://github.com/zehuichen123/">
             Github
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="https://drive.google.com/file/d/1KuLvfYPDnHv6cTE2BvoeSg9xbtK7TA2Q/view">
             Resume
           </a>
        
      </nav>

      <main class="tl f6 relative pa4 pa5-ns overflow-hidden" style="padding-top: 2rem !important; padding-bottom: 2rem">
        
          <div class="mb4">
            <div class="fw600 light-silver mt1">09 Aug 2019</div>
            <h1 class="ttu f3 mt0 lh-title cb mb2">
              
              Segmentation Related Models and Loss
            </h1>
            
              <!-- <a href="https://twitter.com/share" class="twitter-share-button">Tweet</a>
              <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <div class="fb-like" data-send="false" data-layout="button_count" data-width="100" data-show-faces="false" data-font="arial" data-action="like"></div> -->
              <!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5a57a2d5807d68ea"></script>
            
          </div>
        
        <div class="markdown-body">
          <p>In this post, we will cover about some techniques about image segmentation, from FCN to DeepLab, from cross entropy loss to lovász-softmax loss and so on.</p>

<hr />

<h2 id="models">Models</h2>

<h3 id="fully-convolutional-net">Fully Convolutional Net</h3>

<p>The first model is Fully Convolutional Nets(FCN) [1]. The main contribution of FCN can be three folds:</p>

<ul>
  <li>Without any fully connected layers.</li>
  <li>Enlarge the size of image: deconvolutional layer.</li>
  <li>Skip structure; ensemble various depth features.</li>
</ul>

<p><img src="/images/segment_topics/fcn.png" /></p>

<p>Without using any DNN layers, FCN allows us to receive different size of images. Besides, FCN provides us with some upsampling methods, since we need to enlarge the size of feature map into original image size.</p>

<blockquote>
  <h3 id="upsample">Upsample</h3>

  <p>In the application of computer vision, after the feature extraction by CNN, the output data will usually become smaller (mainly due to pooling operation). However, sometimes we want the size of our output images can be exactly the same as input images. Hence, the operation to enlarge the size of image into higher resolution projection is called upsample.</p>
</blockquote>

<p>Here, we only focus on Transposed convolution. Actually, deconvolution is a special convolutional operation. If we defold the input images as one dimensional vector $X$ and the output images as the same $Y$, the convolutional operation can be represented as $Y = CX$, which is to say, the deconvolution can be rewriten as $X = C^TY$. Finally, what we do is to pad zeros around center points and perform new convolutional operations.</p>

<hr />

<p>As we mentioned before, deconvolutional operation can not extactly recover original information, even with the same filter parameters(actually is a non-inversable). This means FCN is unable to maintain positional information for each pixel. There are two totally different ways to tackle with this issue:</p>

<ul>
  <li>Encoder-Decoder Framework, and enable shortcut connection between encoder and decoder. U-Net is one of those classical models.</li>
  <li>Apply dilated/atrous convolutions, which enables us to remove pooling layers.</li>
</ul>

<hr />

<h3 id="u-net">U-Net</h3>

<p>The structure of U-Net[2] is simple as well.</p>

<p><img src="/images/segment_topics/unet.png" /></p>

<p>Briefly, there are two main points that makes U-Net peforms better than FCN, IMO.</p>

<p>Firstly, FCN applys summeration for multiple feature maps while U-Net chooses concatenation. Mathematically, summeration will lose information, right?</p>

<p>Another point is concatenation enables U-Net to consider more global, but worse resolution as well as more local but better resolution information.</p>

<hr />

<h3 id="deeplab-v3">DeepLab v3</h3>

<p>DeepLab[3] removes pooling layers and replaces it with dialated convolutions. The dialated/atrous convolution can be illustrated as following figures:</p>

<center><img src="/images/segment_topics/deeplab_1.png" height="200" /></center>

<center><i>Atrous convolution with kernel size 3 × 3 and different rates. Standard convolution corresponds to atrous convolution with rate = 1. Employing large value of atrous rate enlarges the model’s field-of-view, enabling object encoding at multiple scales.</i></center>

<p>The main contribution of DeepLabv3 System is listed as follows:</p>

<ul>
  <li>employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple scales.</li>
  <li>Advanced <em>Atrous Spatial Pyramid Pooling</em> module, which probe convolutional features at mutiple scales, with image-level features encoding global context.</li>
</ul>

<p>The above points are proposed to deal with two challenges:</p>

<ul>
  <li>Multiple Pooling or convolution worse the feature resolution, which leads to the uncertainty of positional information.</li>
  <li>Multiple scale objects in the same images.</li>
</ul>

<p><img src="/images/segment_topics/deeplab_2.png" /></p>

<p><img src="/images/segment_topics/deeplab_3.png" /></p>

<center><i>Parallel modules with atrous convolution (ASPP), augmented with image-level features.</i></center>

<hr />

<h3 id="some-thoughts-about-these-models">Some Thoughts about These Models</h3>

<p>For most Kaggle image segmentation competitions, you will find U-Net is probably the most popular framework , not one of. The reason of this can be concluded:</p>

<ul>
  <li>U-Net itself is one competitve model in image segmentation.</li>
  <li>Actually, U-Net provides us with a extendable framework, an encoder-decoder framework, which enables us to replace the encoder/decoder structure with different architecture, like ResBlock, SENet or EffientNet. The easiness of U-Net to ensemble other advanced framework is probably the most important reason that makes it so popular.</li>
</ul>

<hr />

<h2 id="loss">Loss</h2>

<p>The loss in image segmentation is also an important topic. The extremely imbalanced positive dataset poses great challenges to our training phase. A carefully designed loss can benefit you with better performance.</p>

<h3 id="log-loss">Log Loss</h3>

<p>The log loss is actually binary cross entropy loss, which is widely used in binary classificaiton tasks. The formula can be written as</p>

<script type="math/tex; mode=display">L = -y \cdot log(y') - (1-y)\cdot log(1-y)</script>

<p>However, this loss function holds an evident drawback: when the positive samples are much less than negative samples, the model can not learn very well from positive samples due to lack of information.</p>

<h3 id="dice-loss">DICE Loss</h3>

<p>Firstly, we will define the similarity between two shapes. We use A, B to denote the points inside these two shapes. Then,</p>

<script type="math/tex; mode=display">DSC(A, B) = 2 \lvert A \cap B\lvert / \lvert A \cup B \lvert</script>

<p>So the loss can be written as</p>

<script type="math/tex; mode=display">DL_2 = 1 - \frac{\sum_{n=1}^Np_n r_n + \epsilon}{\sum_{n=1}^N p_n + r_n + \epsilon} - \frac{\sum_{n=1}^N(1-p_n)(1-r_n)+\epsilon}{\sum_{n=1}^N2-p_n-r_n + \epsilon}</script>

<p>Here is an implementation with Keras</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dice_coef</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
  <span class="n">intersaction</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
  <span class="n">union</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span> <span class="o">+</span> <span class="n">K</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">intersection</span> <span class="o">+</span> <span class="n">smooth</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">union</span> <span class="o">+</span> <span class="n">smooth</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dice_coef_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">dice_coef</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="focal-loss">Focal Loss</h3>

<script type="math/tex; mode=display">% <![CDATA[
f(n) = \begin{cases} -(1-y')^\gamma \text{log}y', &\text{if y=1} \\ -y'^\gamma\text{log}(1-y'), & \text{if y=0}\end{cases} %]]></script>

<p>In focal loss, we focus more on samples which is hard to classify, and assign low penalty to those samples which is easier. Take $\gamma = 2 $ as an example,</p>

<ul>
  <li>for positive samples, if our prediction is 0.97, then it must be an easy-to-classify sample, hence $(1-0.97)^\gamma$ will be very small. On the contrary, if our prediction is 0.3, then it would be a hard-to-classify sample, then $(1-0.3)^\gamma$ will be quite large (at least larger than the previous one)</li>
  <li>vice versa</li>
</ul>

<p>Additionally, focal loss also use $\alpha$, called balance factor to balance the distribution between positive and negative samples.</p>

<script type="math/tex; mode=display">% <![CDATA[
f(n) = \begin{cases} -\alpha (1-y')^\gamma \text{log}y', &\text{if y=1} \\ -(1-\alpha)y'^\gamma\text{log}(1-y'), & \text{if y=0}\end{cases} %]]></script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">focal_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.75</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.25</span>
  <span class="n">pt_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span>
  <span class="n">pt_0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span>
  
  <span class="n">pt_1</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">pt_1</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">.999</span><span class="p">)</span>
  <span class="n">pt_2</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">pt_0</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">.999</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="o">-</span><span class="n">K</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">pt_1</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pt_1</span><span class="p">))</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">pt_0</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">pt_0</span><span class="p">))</span>
  
</code></pre></div></div>

<h3 id="lovasz-softmax-loss">Lovasz-Softmax Loss</h3>

<p>IoU (Jaccard index) is actually intractable, since it needs equal operation. However, Jaccard loss can be performed with Lovasz extension, which enables discrete space into continuous space, which will be tractable.</p>

<p>BTW, lovasz-softmax loss is used during fine-tuning phase. A common practice is to train the model with BCE loss/ DICE loss a few epochs and then turn the loss to Lovasz loss + DICE/BCE loss.</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">Fully Convolutional Networks for Semantic Segmentation</a></p>

<p>[2] <a href="https://arxiv.org/abs/1505.04597">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p>

<p>[3] <a href="https://arxiv.org/abs/1706.05587">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p>

<p>[4] <a href="www.kaggle.com">Kaggle: A Data Competition Platform</a></p>


        </div>
        <p class="mt4" style="font-family: 'Sens Serif Pro'">
  End of Post<br>
  <span class="silver">at 13:43</span>
</p>
<img src="http://localhost:4000/images/scribble3.png" alt="scribble" />

      </main>
      <section class="fixed-l mw7 center w-100 top-50 tc pb4 nt4" style="max-width: 58rem">
  
    <a href="http://localhost:4000/posts/revisit-variational-autoencoder" class="no-underline f1 light-blue hover-silver nl5 fl-l ph3">‹</a>
  
  
    <a href="http://localhost:4000/posts/gbdt-explanation" class="no-underline f1 light-blue hover-silver nr5 fr-l ph3">›</a>
  
</section>
    </div>
    <footer class="mw7 center tc pt3 pb4 silver">
      Built with Jekyll using <a href="http://github.com/muan/scribble" class="link silver hover-blue pv1">Scribble</a>.
      <img src="http://localhost:4000/images/scribble2.png" alt="scribble" class="mt4 db center" />
    </footer>
  </body>
</html>
