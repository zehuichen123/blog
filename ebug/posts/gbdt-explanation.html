
<!doctype>
<html lang="en">
  <head>
    <meta content='Gradient Boosting Decision Tree Explanation - Zehui Chen' name='title' />
    <meta content='Gradient Boosting Decision Tree Explanation - Zehui Chen' name='og:title' />
    <title>Gradient Boosting Decision Tree Explanation - Zehui Chen</title>
    <link href='http://localhost:4000/images/fav.png' rel='shortcut icon'>
<link href='http://localhost:4000/stylesheets/style.css' rel='stylesheet' type='text/css' />
<link href='http://localhost:4000/stylesheets/syntax.css' rel='stylesheet' type='text/css' />

<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type' />

  <meta content='http://localhost:4000/posts/gbdt-explanation' property='og:url' />
  <meta content="GBDT(Gradient Boosting Decision Tree) is a kind of iterative algorithm. It consists of multiple decision tree, and th..." property='og:description' />
  <meta content="article" property="og:type" />

<!-- - -->





    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$','$']]
        }
      });
    </script> <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </head>
  <body class="lh-copy dark-gray pa0 f6 sans-serif bg-super-white">
    <header class="tc mt4">
      <a href="http://localhost:4000">
        <img src="http://localhost:4000/images/new.jpg" alt="Home" width="70" height="70" style="border-radius:50%;">
      </a>
      <p>Zehui Chen</p>
    </header>
    <div class="mw8 bg-white mt4 mb3 center br2-ns bt bb ba-ns b--light-gray" style="max-width: 59rem">
      <nav class="bb b--light-gray pv4 tc" aria-label="Main">
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/about">
             About
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/">
             Blog
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/friends">
             Friends
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="https://github.com/zehuichen123/">
             Github
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="https://drive.google.com/file/d/1KuLvfYPDnHv6cTE2BvoeSg9xbtK7TA2Q/view">
             Resume
           </a>
        
      </nav>

      <main class="tl f6 relative pa4 pa5-ns overflow-hidden" style="padding-top: 2rem !important; padding-bottom: 2rem">
        
          <div class="mb4">
            <div class="fw600 light-silver mt1">12 Aug 2019</div>
            <h1 class="ttu f3 mt0 lh-title cb mb2">
              
              Gradient Boosting Decision Tree Explanation
            </h1>
            
              <!-- <a href="https://twitter.com/share" class="twitter-share-button">Tweet</a>
              <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <div class="fb-like" data-send="false" data-layout="button_count" data-width="100" data-show-faces="false" data-font="arial" data-action="like"></div> -->
              <!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5a57a2d5807d68ea"></script>
            
          </div>
        
        <div class="markdown-body">
          <p>GBDT(Gradient Boosting Decision Tree) is a kind of iterative algorithm. It consists of multiple decision tree, and the result is derived from these trees. In this post, we will cover the following parts from intuitive way, in other words, not so mathematical :)</p>

<ul>
  <li>Decision Tree(Regression DT and Classification DT)</li>
  <li>Bagging and Boosting</li>
  <li>An Example of GBDT</li>
  <li>Difference between GBDT and Adaboost</li>
  <li>Shrinkage</li>
</ul>

<hr />

<h2 id="decision-tree">Decision Tree</h2>

<p>When it comes to decision tree, most people may think about C45 classification tree. However, GBDT doesn’t use classification DT as its base tree, but the regression decision tree. The reason of this is that GBDT is to learn the residual of the prediction, and probably CDT can not learn such a uncertainty value which enables us to sum it up. So what’s the difference between CDT and RDT?</p>

<p>For CDT, it iteratively choose different threshold for each feature, and find the one which yields the best improvement for entropy(maximize entropy here can be understood as let each node only holds one kind of instance). In most cases, we may not let every node holds only one kind instance, hence early stopping is introduced (For example the node can not be divided because it must hold at least $n$ instances).</p>

<p>The process of RDT is similar to CDT, however, for each node we can compute the prediction value. Take Age prediction as example, the prediction value is equal to the average of all instances which are assigned to this node. What’s more, the judge value/ objecitve function is not the entropy but the prediciton error.</p>

<hr />

<h2 id="bagging-and-boosting">Bagging and Boosting</h2>

<p>Bagging and boosting are probably the most important topics in ensemble methods. Random forests is the representation of bagging. The main intuition is to randomly select samples from dataset and train your model. When obtaining plentiy of models, just average them up and you will get your answer (Ok, this is not the formal definition of Bagging, actually, it is a part of how random forests works). In view of our topic is GBDT, I won’t spend much ink on Bagging. So what’s boosting? Boosting is also utilizing multiple base learner to predict but totally different. The base learner inside GBDT is not to learn the final prediction value but the residual value of the previous base learners. Take age prediction as example, say A’s true age is 16. If the first base learner predict A’s age is 10 then the second learner is to learn the residual value of true age and the previous prediction value (16 - 10 = 6).</p>

<hr />

<h2 id="example-of-gbdt">Example of GBDT</h2>

<p>Let’s take age prediction as an example, yeah, I love this example. Say we have 4 people A,B,C,D and their ages 14, 16, 24, 26. If we choose a DT to predict the age, the result may probably looks like this:</p>

<center><img src="/images/gbdt/dt.png" height="300" /></center>
<center><i>An illustraction of an decision tree.</i></center>
<p>Then we utilize GBDT to learn this task, and we limit the maximum number of nodes is 2, and n_estimators=2, then we may get this figure.</p>

<center><img src="/images/gbdt/gbdt.png" /></center>
<center><i>An illustraction of an GBDT.</i></center>
<p>Note that in the second tree, it not learn the value of 14, 16, 24, 26 but the residual value from the previous base learners.</p>

<hr />

<h2 id="gbdt-and-adaboost">GBDT and AdaBoost</h2>

<p>Adaboost and GBDT are both boosting model. So what’s the difference between them?</p>

<p>For Adaboost, it compute the weight for each instance based on the previous prediction, and assign more weight on those which are wrongly classified and assign low penalty to those classified correctly. Actually, the intuition of Bootstrap and Adaboost are the same. But what about GBDT? GBDT is to learn the residual function of previous prediction. In some ways, you can think about it as learning something that is hard to learn.</p>

<hr />

<h2 id="shrinkage">Shrinkage</h2>

<p>Shrinkage can be viewed as a competitive regularization method for boosting. The idea behind shrinkage is more little steps towards final result is better/less likely to overfitting than large steps towards optimal. It just assure that each tree may only learn part of truth. And multiple trees can learn better.</p>

<p>No Shrinkage:</p>
<center>
$$
y_{n+1} \to residual(\sum_{i=1}^n y_i)\\
y_{pred} = sum(y1,..., y_{n+1})
$$
</center>
<p>With shrinkage:</p>
<center>
$$
y_{n+1} \to residual(\sum_{i=1}^n y_i)\\
y_{pred} = sum(y1,..., y_{n}) + step *y_{n+1}
$$
</center>

        </div>
        <p class="mt4" style="font-family: 'Sens Serif Pro'">
  End of Post<br>
  <span class="silver">at 13:12</span>
</p>
<img src="http://localhost:4000/images/scribble3.png" alt="scribble" />

      </main>
      <section class="fixed-l mw7 center w-100 top-50 tc pb4 nt4" style="max-width: 58rem">
  
    <a href="http://localhost:4000/posts/segmentation-related" class="no-underline f1 light-blue hover-silver nl5 fl-l ph3">‹</a>
  
  
    <a href="http://localhost:4000/posts/xgboost-explanation" class="no-underline f1 light-blue hover-silver nr5 fr-l ph3">›</a>
  
</section>
    </div>
    <footer class="mw7 center tc pt3 pb4 silver">
      Built with Jekyll using <a href="http://github.com/muan/scribble" class="link silver hover-blue pv1">Scribble</a>.
      <img src="http://localhost:4000/images/scribble2.png" alt="scribble" class="mt4 db center" />
    </footer>
  </body>
</html>
