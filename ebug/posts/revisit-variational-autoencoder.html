
<!doctype>
<html lang="en">
  <head>
    <meta content='Revisiting Variational Autoencoder - Zehui Chen' name='title' />
    <meta content='Revisiting Variational Autoencoder - Zehui Chen' name='og:title' />
    <title>Revisiting Variational Autoencoder - Zehui Chen</title>
    <link href='http://localhost:4000/images/fav.png' rel='shortcut icon'>
<link href='http://localhost:4000/stylesheets/style.css' rel='stylesheet' type='text/css' />
<link href='http://localhost:4000/stylesheets/syntax.css' rel='stylesheet' type='text/css' />

<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type' />

  <meta content='http://localhost:4000/posts/revisit-variational-autoencoder' property='og:url' />
  <meta content="Given examples X distributed according  to some unknown distribution $P_{gt}(X)$, the goal is to learn a model $P$ wh..." property='og:description' />
  <meta content="article" property="og:type" />

<!-- - -->





    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$','$']]
        }
      });
    </script> <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </head>
  <body class="lh-copy dark-gray pa0 f6 sans-serif bg-super-white">
    <header class="tc mt4">
      <a href="http://localhost:4000">
        <img src="http://localhost:4000/images/new.jpg" alt="Home" width="70" height="70" style="border-radius:50%;">
      </a>
      <p>Zehui Chen</p>
    </header>
    <div class="mw8 bg-white mt4 mb3 center br2-ns bt bb ba-ns b--light-gray" style="max-width: 59rem">
      <nav class="bb b--light-gray pv4 tc" aria-label="Main">
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/about">
             About
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/">
             Blog
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="http://localhost:4000/friends">
             Friends
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="https://github.com/zehuichen123/">
             Github
           </a>
        
          <a class="link link-title blue hover-mid-gray mh2 pv1" style="color: #1d5884 !important; font-size: 13px;"
             href="https://drive.google.com/file/d/1KuLvfYPDnHv6cTE2BvoeSg9xbtK7TA2Q/view">
             Resume
           </a>
        
      </nav>

      <main class="tl f6 relative pa4 pa5-ns overflow-hidden" style="padding-top: 2rem !important; padding-bottom: 2rem">
        
          <div class="mb4">
            <div class="fw600 light-silver mt1">02 Aug 2019</div>
            <h1 class="ttu f3 mt0 lh-title cb mb2">
              
              Revisiting Variational Autoencoder
            </h1>
            
              <!-- <a href="https://twitter.com/share" class="twitter-share-button">Tweet</a>
              <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <div class="fb-like" data-send="false" data-layout="button_count" data-width="100" data-show-faces="false" data-font="arial" data-action="like"></div> -->
              <!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5a57a2d5807d68ea"></script>
            
          </div>
        
        <div class="markdown-body">
          <p>Given examples X distributed according  to some unknown distribution $P_{gt}(X)$, the goal is to learn a model $P$ which we can sample from, such that $P$ is as similiar as possible to $P_{gt}$.</p>

<hr />

<h2 id="drawbacks-of-existed-methods">Drawbacks of existed methods</h2>

<ul>
  <li>require strong assumptions about the structure in the data</li>
  <li>make severe approximations, leading to suboptimal models</li>
  <li>rely on computationally expensive inference procedures like Markov Chain Monte Carlo.</li>
</ul>

<hr />

<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>

<p>Beforer starting VAE, we would like to revisit the relationship between maximum likelihood and KL divergence. So, if we are interested in estimating a true distribution $p(x)$, we introduce a set of candidate distributions $\mathcal{P}_x$. Hence, though we don’t have an access to the true distirbution $p^*(x)$, we do have the access to finite samples from $\mathcal{P}_x$. We denote uniform sampling from this finite dataset as $\hat{p}(x)$. The goal of maximum likelihood estimation is to find the best $p \in \mathcal{P}_x$ to approximate $p^(x)$ as measured by the Kullback-Leibler divergence,</p>

<script type="math/tex; mode=display">\min \limits_{p\in\mathcal{P}_x} D(\hat{p}||p_g) = \min \limits_{p\in\mathcal{P}_x}\mathbb{E}_{\hat{p}(x)} \displaystyle \left[{\rm In}\frac{\hat{p}(x)}{p(x)}\right] \\ \equiv \max_{p\in \mathcal{P}_x}\mathbb{E}_{\hat{p}(x)}[{\rm In} p(x)]</script>

<p>From the above, we can find that mimizing the KL divergence is equalvalent to maximizing the log likelihood.</p>

<hr />

<h2 id="goal">Goal</h2>

<p>Maximize the probability of each X in the training set under the entire generative process,</p>

<script type="math/tex; mode=display">P(X) = \int P(X|z; \theta)P(z)dz</script>

<ul>
  <li>
    <p>In terms of the shape of $z$, Gaussian distribution is ok since any distribution in $d$ dimension can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function.</p>
  </li>
  <li>
    <p>The second problem is to maximize $P(X)$. An intuitive idea is sample points $z_i$ from $P(z)$ and then to approximate $P(X)$ with</p>

    <script type="math/tex; mode=display">\sum_{i=0}^{n}P(X|z_i)</script>

    <p>However, this is highly compuationally. VAE alters the sampling procedure.</p>
  </li>
</ul>

<hr />

<h2 id="most-interesting-part">Most Interesting Part</h2>

<h3 id="setting-the-objective">Setting the objective</h3>

<p>For most $z$, $P(X\lvert z)$ provide no information to our estimate of $P(X)$. The key idea of VAE is to sample values of $z$ that are likely to have produces $X$ and compute $P(X)$ just from those. More specially, we need a new function $Q(z\lvert X)$ which can take a value of $X$ and give us a distribution over $z$. Since the space of $z$ values that are likely under $Q$ is much smaller than the space of all $z$ s that are under the prior $P(z)$. Therefore, our task becomes to compute $E_{z	\in Q}P(X\lvert z)$ and relate it with $P(X)$.</p>

<p>The relationship between $E_{z\in Q}P(X\lvert z)$ and $P(X)$ can be measured by KL divergence or $\mathcal{D}$.</p>

<script type="math/tex; mode=display">\mathcal{D}[Q(z)||P(z|X)] = E_{z\in Q}[{\rm log}Q(z) - {\rm log}P(z|X)]</script>

<p>By applying Bayes rule to $P(z\lvert X)$:</p>

<script type="math/tex; mode=display">\mathcal{D}[Q(z)||P(z|X)] = E_{z\in Q}[{\rm log}Q(z) - {\rm log}P(X|z) - {\rm log}P(z)] + {\rm log}P(X)</script>

<p>Negating both sides, rearranging, we can get:</p>

<script type="math/tex; mode=display">{\rm log}P(X) - \mathcal{D}[Q(z)||P(z|X)] = E_{z\in Q}[{\rm log}P(X|z)] - \mathcal{D}[Q(z)||P(z)]</script>

<p>Since we are interested in infering $P(X)$, it makes sense to construct a $Q$ which does depend on $X$, and in particular, one which makes $\mathcal{D}[Q(z)\lvert \lvert P(z\lvert X)] $ small:</p>

<script type="math/tex; mode=display">{\rm log}P(X) - \mathcal{D}[Q(z|X)||P(z|X)] = E_{z\in Q}[{\rm log}P(X|z)] - \mathcal{D}[Q(z|X)||P(z)]</script>

<p>Until now, we reach the core of the variational autoencoder. The left term have the quantity we want to maximize: ${\rm log}P(X)$ plus an error term. The right hand is something that we can optimize through stochastic gradient descent. Let have a deeper look at the rigth hand term.</p>

<p>$E_{z\in Q}[{\rm log}P(X\lvert z)]$ enforces that we want the conditional X can be well reconstructed with sampled $z$, besides, $z$ was generated or sampled from distribution $Q$. And $\mathcal{D}[Q(z\lvert X)\lvert \lvert P(z)]$ is to force the Q to be similar to P. If we put these constraints into an autoencoder framework, it is quite natural to view  $P$ as decoder and $Q$ as encoder.</p>

<h3 id="optimizing-the-objective">Optimizing the objective</h3>

<p>Firstly, what the form that $Q(z\lvert X)$ will take? The usual choice is to say that $Q(z\lvert X)$ follows an Gaussian distribution $\mathcal{N}(z\lvert \mu(X,\theta), \sum(X, \theta))$. Since we have already assumed that $P(z)$ can be viewed as an Gaussian distribution, the last term $\mathcal{D}[Q(z\lvert X)\lvert\lvert P(z)]$ is now a KL-divergence between two Gaussian distribution.</p>

<script type="math/tex; mode=display">\mathcal{D}[\mathcal{N}(\mu(X), \Sigma(X))||\mathcal{N}(0, I)] \\=\frac{1}{2}(tr(\Sigma(X))+(\mu(X))^T(\mu(X)) - k - {\rm log}det(\Sigma(X)))</script>

<p>where $k$ is the dimensionally of the distribution.</p>

<p>Suppose we are doing stochastic gradient descent over different values of $X$ sampled from a dataset $D$, the full equation here can be rewrited as:</p>

<script type="math/tex; mode=display">E_{X\in D}[{\rm log}P(X) - \mathcal{D}[Q(z\lvert X)\lvert \lvert P(z \lvert X)]] = \\ E_{X\in D}[E_{z\in Q}[{\rm logP(X\lvert z)}] - \mathcal{D}[Q(z\lvert X)\lvert \lvert P(Z)]]</script>

<p>If we compute the gradient of the right term, $Q(z\lvert X)$ is  untractable. Though we may sample $z$ for forward propagation but this sampling can not propogate back, which is a non-continuous operation and has no gradient. To tackle with this issue, the solution, called “reparameterization trick” was proposed to move the sampling into an input layer. Given $\mu(X)$ and $\Sigma(X)$, we can sample from $\mathcal{N}(\mu(X), \Sigma(X))$ by first sampling $\epsilon$ from $\mathcal{N}(0, I)$, then computing $z = \mu(X)+\Sigma^{1/2}(X)\cdot \epsilon$.</p>

<p><img src="/images/revisiting_vae/vae.png" /></p>

<center><i>Figure: A training-time variational autoencoder implemented as a feedforward neural network, where $P(X\lvert z)$ is Gaussian. Left is without the “reparameterization trick”, and right is with it. Red shows sampling operations that are non-differentiable. Blue shows loss layers. The feedforward behavior of these networks is identical, but backpropagation can be applied only to the right network.</i></center>
<hr />

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a>(https://arxiv.org/abs/1606.05908)</p>

<p>[2] <a href="http://ruishu.io/2018/03/14/vae/">DENSITY ESTIMATION: VARIATIONAL AUTOENCODERS</a> (http://ruishu.io/2018/03/14/vae/)</p>

        </div>
        <p class="mt4" style="font-family: 'Sens Serif Pro'">
  End of Post<br>
  <span class="silver">at 16:52</span>
</p>
<img src="http://localhost:4000/images/scribble3.png" alt="scribble" />

      </main>
      <section class="fixed-l mw7 center w-100 top-50 tc pb4 nt4" style="max-width: 58rem">
  
  
    <a href="http://localhost:4000/posts/segmentation-related" class="no-underline f1 light-blue hover-silver nr5 fr-l ph3">›</a>
  
</section>
    </div>
    <footer class="mw7 center tc pt3 pb4 silver">
      Built with Jekyll using <a href="http://github.com/muan/scribble" class="link silver hover-blue pv1">Scribble</a>.
      <img src="http://localhost:4000/images/scribble2.png" alt="scribble" class="mt4 db center" />
    </footer>
  </body>
</html>
